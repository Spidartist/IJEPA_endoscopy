INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
INFO:root:making polyp data transforms
INFO:root:Polyp dataset created
INFO:root:Polyp unsupervised data loader created
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:Set up meters for epoch 1
0it [00:00, ?it/s]
INFO:root:Iteration 1
INFO:root:[1,     0] loss: 0.487 masks: 77.0 42.0 [wd: 3.00e-02] [lr: 4.00e-04] [mem: 2.13e+04] (4592.3 ms)
INFO:root:[1,     0] grad_stats: [4.42e-02 1.94e-02] (1.89e-02, 4.82e-02)


2it [00:12,  5.47s/it]
INFO:root:[1,     1] loss: 0.427 masks: 73.0 42.0 [wd: 3.00e-02] [lr: 4.00e-04] [mem: 2.13e+04] (3415.7 ms)
INFO:root:[1,     1] grad_stats: [2.35e-03 2.08e-03] (1.92e-03, 3.53e-03)

3it [00:14,  3.91s/it]
INFO:root:[1,     2] loss: 0.389 masks: 70.7 44.0 [wd: 3.00e-02] [lr: 4.00e-04] [mem: 2.13e+04] (2959.1 ms)
INFO:root:[1,     2] grad_stats: [1.16e-03 5.63e-04] (5.53e-04, 1.83e-03)
3it [00:16,  5.41s/it]
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa520423310>
Traceback (most recent call last):
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1443, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/mnt/quanhd/env/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/mnt/quanhd/env/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/mnt/quanhd/env/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/mnt/quanhd/env/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3468362) is killed by signal: Aborted.
Process Process-1:
Traceback (most recent call last):
  File "/mnt/quanhd/env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/mnt/quanhd/env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/quanhd/ijepa/main_polyp.py", line 52, in process_main
    app_main(args=params)
  File "/mnt/quanhd/ijepa/src/train_polyp.py", line 370, in main
    (loss, _new_lr, _new_wd, grad_stats, rm), etime = gpu_timer(train_step)
  File "/mnt/quanhd/ijepa/src/utils/logging.py", line 21, in gpu_timer
    result = closure()
  File "/mnt/quanhd/ijepa/src/train_polyp.py", line 346, in train_step
    z = forward_context()
  File "/mnt/quanhd/ijepa/src/train_polyp.py", line 333, in forward_context
    z = predictor(z, masks_enc, masks_pred)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/quanhd/ijepa/src/models/vision_transformer.py", line 341, in forward
    x = blk(x)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/quanhd/ijepa/src/models/vision_transformer.py", line 184, in forward
    y, attn = self.attn(self.norm1(x))
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/quanhd/env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/quanhd/ijepa/src/models/vision_transformer.py", line 162, in forward
    attn = attn.softmax(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 105.12 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)